收集大规模梯度下降优化的相关资料，主要参考目录是这篇博客[An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/index.html)。根据这篇文章，大规模梯度下降优化主要有两个方向：1）学习率自适应；2）下降路径优化。

sgd Momentum 不稳定，但是好的时候，收敛速度比gd还快。

Nesterov accelerated gradient的论文无法搜索到，先看看Momentum的论文。
